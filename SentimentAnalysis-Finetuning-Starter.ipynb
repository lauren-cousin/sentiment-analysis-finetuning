{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install dependencies"
      ],
      "metadata": {
        "id": "oTaCz1i7WojQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_puNMeUsx2p"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Example: Run sentiment analysis predictions by using Pipeline\n",
        "Without specifying a model name, HuggingFace uses the default model, which is `distilbert-base-uncased-finetuned-sst-2-english`."
      ],
      "metadata": {
        "id": "YXM_PsOvWrrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pipeline class to make predictions from models available in the Hub in an easy way\n",
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "data = [\"I love you\", \"I hate you\"]\n",
        "sentiment_pipeline(data)"
      ],
      "metadata": {
        "id": "IVAp8Urhs3Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Example: Select a sentiment analysis model for our fine-tuning task\n",
        "We want to pick a base model that is small enough to train in Colab with free resources and see some results after fine-tuning. We will select a couple to evaluate based on likes, number of downloads, and size.\n",
        "- The `finiteautomata/bertweet-base-sentiment-analysis` model has 135 million parameters. It is based on BERTweet, which is a RoBERTa model pre-trained on 850M English tweets. It is specifically fine-tuned for sentiment analysis on social media text.\n",
        "- The `distilbert-base-uncased-finetuned-sst-2-english` model is the default model provided by HuggingFace (that we tried out above). It has 66 million parameters and uses DistilBERT, a smaller, faster, and cheaper version of BERT.\n",
        "\n",
        "Since both models are small and efficient, we'll try out `finiteautomata/bertweet-base-sentiment-analysis`. This model has POSITIVE, NEGATIVE, and NEUTRAL labels, instead of only POSITIVE/NEGATIVE. This time we will pull it into our code from the HuggingFace Models Hub by name. Note that our values will look slightly different since we're using a new model.\n",
        "\n",
        "**Model Cards:**\n",
        "\n",
        "\n",
        "*   https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
        "*   https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "**Tip:** If you sign up for a free Hugging Face account, you can interact with their Inference API available on model card pages, allowing you to quickly test sample outputs for various inputs."
      ],
      "metadata": {
        "id": "JJmScQF_W1fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Set up our selected model for sentiment analysis\n",
        "sentiment_analysis_model = None\n",
        "# TODO: Call the model on with passed in `data` to ensure it's working as expected\n"
      ],
      "metadata": {
        "id": "9s-KqAx1s6qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create test data\n",
        "Now that we have a model in place, let's create some test data that's a little more ambiguous. Add your own!\n",
        "\n"
      ],
      "metadata": {
        "id": "riR0eBIr2m8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    # TODO: Add your own test data\n",
        "    \"That was really uncool\",\n",
        "    \"I feel unsure\",\n",
        "    \"I would rather not work with this company again\",\n",
        "    \"I feel you guys did everything you could\",\n",
        "    \"The movie was okay, not great but not bad either\",\n",
        "    \"I guess it could be worse\",\n",
        "    \"The service was decent but could improve\",\n",
        "    \"I'm not entirely satisfied with the results\",\n",
        "    \"It wasn't what I expected, but it was alright\",\n",
        "    \"Is the store open on the weekends?\",\n",
        "    \"The food was somewhat edible\",\n",
        "    \"I have mixed feelings about this project\",\n",
        "    \"How many lines of credit can I open?\",\n",
        "    \"The presentation was underwhelming\",\n",
        "    \"I suppose it's fine for now\",\n",
        "    \"It's not my favorite, but it's acceptable\",\n",
        "    \"It's summer weather outside\",\n",
        "    \"There are things I like and dislike about it\",\n",
        "    \"I'm neither happy nor upset about the outcome\",\n",
        "    \"It's passable, though not impressive\",\n",
        "    \"The product is okay, but I've seen better\",\n",
        "    \"I can tolerate it, but I don't love it\",\n",
        "    \"It didn't meet my expectations, but it's not terrible\",\n",
        "    \"I'm pretty neutral\",\n",
        "    \"Fairly alright\",\n",
        "    \"The couch is gray\",\n",
        "    \"I hope he eats dirt\",\n",
        "    \"Through all the doom and gloom, he survived in the end, though not unscathed\",\n",
        "    \"I can't believe how invigorating this experience was, never experienced anything like it\",\n",
        "    \"I'm really grateful for the extended hours during the exam season.\",\n",
        "    \"I had a hard time finding a quiet spot due to loud conversations.\",\n",
        "    \"Are there any penalties for returning books late?\"\n",
        "]"
      ],
      "metadata": {
        "id": "LvRGPxrR2mfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Get a baseline\n",
        "Log the sentiment labels and scores from the baseline model for each item in the test data. We will use this for a comparison with our fine-tuned model to ensure its performance is improving."
      ],
      "metadata": {
        "id": "rpTUhzg3AxK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call our sentiment analysis model again, this time with `test_data`, to get sentiment labels and scores\n",
        "results = None\n",
        "\n",
        "# TODO: Print each test data phrase with its corresponding sentiment analysis result for visibility\n",
        "for phrase, result in zip(test_data, results):\n",
        "    print(f\"Sentence: {phrase}\\nSentiment: {result}\\n\")"
      ],
      "metadata": {
        "id": "wsv6duZWA21N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis: Areas for improvement\n",
        "A somewhat subjective analysis:\n",
        "\n",
        "- Likely Neutral:\n",
        "  - \"I feel unsure\" is classified as negative but could be considered more contextually neutral. Its score should reflect more ambiguity.\n",
        "  - \"The movie was okay, not great but not bad either\" should be classified as neutral rather than positive.\n",
        "  - \"I have mixed feelings about this project\" should be classified as neutral rather than negative.\n",
        "  - \"I'm neither happy nor upset about the outcome\" should be classified as neutral rather than negative.\n",
        "  - \"Through all the doom and gloom, he survived in the end, though not unscathed\" should likely be neutral rather than positive.\n",
        "  - \"Are there any penalties for returning books late?\" should be neutral instead of negative.\n",
        "- Neutral/Negative:\n",
        "  - \"The product is okay, but I've seen better\" should be classified as neutral or negative rather than a strong positive.\n",
        "  - \"I can tolerate it, but I don't love it\" may be better as a weaker negative.\n",
        "  - \"The service was decent but could improve\" is classified as positive, but has neutral/negative undertones.\n",
        "- Neutral/Positive:\n",
        "  - \"I can't believe how this experience was, never experienced anything like it\" is classified as negative, but has neutral/positive undertones.\n",
        "  - \"Fairly alright\" is more neutral/positive than positive.\n",
        "- Negative:\n",
        "  - \"The food was somewhat edible\" is classified as positive, but it suggests a negative sentiment.\n",
        "  - \"It didn't meet my expectations, but it's not terrible\" is classified as positive, but it suggests a negative sentiment."
      ],
      "metadata": {
        "id": "FIY646NeEgU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Fine-tune\n",
        "## Load a training dataset\n",
        "Let's try out `yamini0506/hotel_reviews_sentiment_1K`.\n",
        "\n",
        "Dataset card: https://huggingface.co/datasets/yamini0506/hotel_reviews_sentiment_1K\n",
        "\n",
        "This dataset comes pre-split into train, val, and test data. This makes it easy for us. However, sometimes we have to split our data ourselves, which can look something like this:\n",
        "```\n",
        "### EXAMPLE ###\n",
        "### SPLITTING A DATASET ###\n",
        "# Load the dataset\n",
        "dataset = load_dataset('davanstrien/fake-library-chats-with-sentiment')\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "```\n",
        "\n",
        "- **Training dataset**: Used to train the model. The model learns patterns and relationships from this data. Usually the biggest subset of data so it can learn effectively.\n",
        "- **Validation dataset**: Used to tune the model's hyperparameters and for model selection. It helps in evaluating the model's performance during the training phase and in preventing overfitting by providing feedback on the model's generalization ability. Smaller than training dataset. Often, the data is split into 70-80% for training, 10-15% for validation, and 10-15% for testing.\n",
        "- **Testing dataset**: Used to evaluate the final model after training and hyperparameter tuning. It provides an unbiased evaluation of the model's performance on unseen data. Smaller than training dataset.\n",
        "\n",
        "### Tip – Data Integrity:\n",
        "I referenced another dataset above called `davanstrien/fake-library-chats-with-sentiment`. This dataset looks great upon first glance, but I noticed abnormally high accuracy training accuracy/overfitting when I implemented it. Upon further inspection, the data is full of duplicates. It's important to ensure the data you're using is quality and meaningful to get good results. Overfitting = model learns the training data too closely, resulting in it failing to generalize to unseen data.\n",
        "\n",
        "Dataset Card: https://huggingface.co/datasets/davanstrien/fake-library-chats-with-sentiment\n",
        "\n",
        "Below, we can see one method for checking a dataset for overlapping samples."
      ],
      "metadata": {
        "id": "EAEob8rCF1xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from collections import Counter\n",
        "\n",
        "# TODO: Load the `yamini0506/hotel_reviews_sentiment_1K` dataset\n",
        "dataset = None\n",
        "\n",
        "# Inspect the dataset\n",
        "print(dataset)\n",
        "\n",
        "# TODO: Split the dataset into train, validation, and test sets\n",
        "train_dataset = None\n",
        "val_dataset = None\n",
        "test_dataset = None\n",
        "\n",
        "# Inspect the split datasets\n",
        "print(train_dataset)\n",
        "print(val_dataset)\n",
        "print(test_dataset)\n",
        "\n",
        "# Check for overlapping examples\n",
        "train_reviews = set([example['total_review'] for example in train_dataset])\n",
        "test_reviews = set([example['total_review'] for example in test_dataset])\n",
        "overlap = train_reviews.intersection(test_reviews)\n",
        "print(f\"Number of overlapping reviews: {len(overlap)}\")"
      ],
      "metadata": {
        "id": "KoiY291NGSKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the dataset\n",
        "The key piece here is tokenizing our data.\n",
        "\n",
        "* Tokenization is a crucial step in NLP that breaks down text into tokens, enabling models to process and learn from the data.\n",
        "* Tokenizers like AutoTokenizer handle the complex task of converting text into tokens and token IDs, including handling padding, truncation, and special tokens.\n",
        "* Implementation in practice involves initializing a tokenizer, defining a preprocessing function, and applying it to the dataset.\n",
        "\n",
        "Each tokenizer has a vocabulary, a mapping of tokens to unique identifiers. During tokenization, text is converted into these identifiers for the model to process.\n",
        "\n",
        "Example:\n",
        "```\n",
        "tokens = tokenizer.tokenize(\"I love machine learning\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)  # Output: [1045, 2293, 3698, 4086]\n",
        "```"
      ],
      "metadata": {
        "id": "89MtZJQzGkiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# TODO: Set up the AutoTokenizer for our base model\n",
        "tokenizer = None\n",
        "\n",
        "# TODO: Write a preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the input texts\n",
        "    # Use `truncation=True` and `padding=True` in the tokenization process to ensure all input sequences are of consistent length\n",
        "    tokenized_inputs = None\n",
        "    # Add labels to the tokenized inputs\n",
        "    # The Trainer class in transformers library expects 'labels', so we have to remap\n",
        "    tokenized_inputs['labels'] = None\n",
        "    return tokenized_inputs\n",
        "\n",
        "# TODO: Apply the preprocessing function to the train, val, and test datasets\n",
        "# `batched=True` is a performance optimization that allows the tokenizer to process multiple examples at once\n",
        "tokenized_train_dataset = None\n",
        "tokenized_val_dataset = None\n",
        "tokenized_test_dataset = None\n",
        "\n",
        "# Inspect the tokenized dataset\n",
        "print(tokenized_train_dataset)\n",
        "print(tokenized_val_dataset)\n",
        "print(tokenized_test_dataset)\n",
        "print(tokenized_train_dataset[0])\n",
        "print(tokenized_val_dataset[0])\n",
        "print(tokenized_test_dataset[0])"
      ],
      "metadata": {
        "id": "jWoXhZeQGWJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the model using the dataset we loaded\n",
        "\n",
        "Note: At the top, we've pulled in a `compute_metrics` function to help with an accuracy calculation later on during the final evaluation of our model. We pass this into the TrainingArguments when we initialize the Trainer.\n",
        "\n",
        "\n",
        "### Key Terms\n",
        "* **Epoch**: Refers to one complete pass through the entire training dataset. During training, the model updates its weights as it processes the data. An epoch means the model has seen every training example once. Training for multiple epochs allows the model to learn better patterns from the data.\n",
        "  * Example: If you have 1000 training examples and you train for 5 epochs, your model will have processed 5000 examples in total (though some may be seen more than once).\n",
        "* **Batch**: A batch is a subset of the training data. Instead of updating the model weights after every single training example, batches allow the model to update weights after processing a set number of examples. Using batches improves computational efficiency and makes the learning process smoother and more stable.\n",
        "  * Example: If you have 1000 training examples and use a batch size of 100, each epoch will have 10 batches.\n",
        "* **Loss**: A measure of how well the model's predictions match the actual labels. It quantifies the difference between the predicted outputs and the true outputs. The goal of training is to minimize the loss, making the model's predictions as accurate as possible.\n",
        "* **Learning Rate**: Hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. A higher learning rate means the model will change weights more quickly, while a lower learning rate means the model will change weights more slowly. The learning rate determines the speed and quality of the convergence to the minimum loss.\n",
        "* **Weight Decay**:  A regularization technique that involves adding a small penalty to the loss function to discourage large weights. By penalizing large weights, weight decay helps prevent the model from overfitting to the training data. If the model's weights are too large, it might memorize the training data and perform poorly on unseen data. Weight decay helps to keep the weights in check.\n",
        "* **Overfitting**: Overfitting occurs when a machine learning model captures the noise and details in the training data to such an extent that it negatively impacts the model's performance on new, unseen data. A model that overfits will perform exceptionally well on the training data but poorly on validation or test data because it has memorized the training data rather than learning the underlying patterns."
      ],
      "metadata": {
        "id": "wJbuNQRZGov7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import load_metric\n",
        "\n",
        "# Initialize the metric to be used for evaluation\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "# Define a function to compute metrics during evaluation\n",
        "def compute_metrics(p):\n",
        "    # Get the predictions and labels from the inputs\n",
        "    predictions, labels = p\n",
        "    # Convert the predictions to the class with the highest probability\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    # Compute and return the accuracy metric\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# TODO: Load the base model using AutoModelForSequenceClassification\n",
        "# We have 3 sentiment classes (positive, neutral, negative), so pass in 3 labels\n",
        "\n",
        "# TODO: Initialize training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=None,  # Directory to save the model and other outputs\n",
        "    eval_strategy=None,  # Evaluate the model at the end of each epoch\n",
        "    save_strategy=None,  # Save the model at the end of each epoch\n",
        "    logging_strategy=None,  # Log training information at regular intervals\n",
        "    logging_steps=None,  # Log every 10 steps\n",
        "    learning_rate=None,  # Size of optimization step. Here we have a learning rate of 0.00002\n",
        "    per_device_train_batch_size=None,  # Number of examples to process per step during training\n",
        "    per_device_eval_batch_size=None,  # Number of examples to process per step during evaluation\n",
        "    num_train_epochs=None,  # Number of times the model runs through the entire training data. We're using a single epoch for fast train time\n",
        "    weight_decay=None,  # Penalizes large weights to help prevent overfitting. Set on a logarithmic scale (0.1, 0.01, 0.001...)\n",
        ")\n",
        "\n",
        "# TODO: Create a Trainer instance with the specified training arguments and datasets\n",
        "trainer = Trainer(\n",
        "    model=None,  # The model to be trained\n",
        "    args=None,  # Training arguments defined above\n",
        "    train_dataset=None,  # The dataset to be used for training\n",
        "    eval_dataset=None,  # The dataset to be used for evaluation\n",
        "    tokenizer=None,  # The tokenizer used for preprocessing the data\n",
        "    compute_metrics=None  # The function to compute metrics during evaluation\n",
        ")\n",
        "\n",
        "# TODO: Train the model\n",
        "\n",
        "# TODO: Save the model and tokenizer after training"
      ],
      "metadata": {
        "id": "PkF1ArlgGrsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluate\n",
        "Evaluate both the original model and the new model for accuracy."
      ],
      "metadata": {
        "id": "x0D9JhGuGxvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load the fine-tuned model from the specified directory\n",
        "new_model = None\n",
        "# TODO: Create a pipeline for sentiment analysis using the new model and tokenizer\n",
        "new_model_pipeline = None\n",
        "\n",
        "# TODO: Get predictions from the original model\n",
        "# Note: 'sentiment_analysis_model' is the pipeline for the original model\n",
        "old_results = None\n",
        "# TODO: Get predictions from the fine-tuned model\n",
        "# Note: 'new_model_pipeline' is the pipeline for the fine-tuned model\n",
        "new_results = None\n",
        "\n",
        "# Compare the results for our test data\n",
        "for phrase, old_result, new_result in zip(test_data, old_results, new_results):\n",
        "    print(f\"Sentence: {phrase}\\nOld Model: {old_result}\\nNew Model: {new_result}\\n\")"
      ],
      "metadata": {
        "id": "YHfO6qF-qkMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where we test the accuracy of the model using the `compute_metrics` function we defined earlier.\n",
        "\n",
        "We use the `test` data split from our dataset this time."
      ],
      "metadata": {
        "id": "CvI8hm5nW9qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load the original model from HuggingFace model hub\n",
        "# The model has 3 sentiment classes (positive, neutral, negative), so we pass in 3 labels\n",
        "original_model = None\n",
        "\n",
        "# TODO: Initialize training arguments for evaluating the original model\n",
        "original_training_args = TrainingArguments(\n",
        "    output_dir=None,\n",
        "    eval_strategy=None,\n",
        "    per_device_eval_batch_size=None,\n",
        ")\n",
        "\n",
        "# TODO: Create a Trainer instance for the original model\n",
        "original_trainer = Trainer(\n",
        "    model=None,\n",
        "    args=None,\n",
        "    eval_dataset=None,\n",
        "    tokenizer=None,\n",
        "    compute_metrics=None,\n",
        ")\n",
        "\n",
        "# TODO: Evaluate the original model using the Trainer\n",
        "original_model_eval_results = None\n",
        "print(\"Original Model Evaluation Results:\", original_model_eval_results)\n",
        "\n",
        "# TODO: Evaluate the fine-tuned model using the Trainer\n",
        "new_model_eval_results = None\n",
        "print(\"New Model Evaluation Results:\", new_model_eval_results)"
      ],
      "metadata": {
        "id": "q4sHYq0TwAkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congrats! You've fine-tuned a sentiment analysis model.\n",
        "There are many training techniques and hyperparameters that we didn't cover, so I encourage you to go out there and explore other ways of fine-tuning and see if you can improve the model even more.\n",
        "\n",
        "#### Additional Reading Recommendations:\n",
        "* https://rentry.org/llm-training\n",
        "* https://levelup.gitconnected.com/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc\n",
        "* https://addepto.com/blog/rag-vs-fine-tuning-a-comparative-analysis-of-llm-learning-techniques/"
      ],
      "metadata": {
        "id": "uid8xYQyrZSf"
      }
    }
  ]
}